---
title: "Predicting employee turnover"
author: "Kate, Meena, Sagar, Swapnil, Zhenfu"
output:
  html_document:
    css: AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: AnalyticsStyles/default.sty
always_allow_html: yes
---



<hr>\clearpage

# The Business Question

What are the characteristics of employees that can help managers predict the possibility of a particular employee leaving the company?

The data is obtained from [Kaggle](https://www.kaggle.com/ludobenistant/hr-analytics).

<hr>\clearpage

# The Process

1. *Part 1*: We cleaned the data to change categorical variables to dummy variables.

2. *Part 2*: We did a factor analysis to identify factors that signal the possibility of an employee leaving.

3. *Part 3*: We did a cluster analysis to define the profile of employees who might leave the company.

Finally, we will use the results of this analysis to make business decisions to reduce the turnover of employees among those who are at the highest risk of leaving the company.


```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```

<hr>\clearpage

# The Data

First we loaded the data to use: `data/HR_comma_sep.csv`.

```{r setupdata1E, echo=FALSE, tidy=TRUE}
# Please ENTER the name of the file with the data used. The file should be a .csv with one row per observation (e.g. person) and one column per attribute. Do not add .csv at the end, make sure the data are numeric.
datafile_name = "data/HR_comma_sep.csv"

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE = 0.5

# Please enter the maximum number of observations to show in the report and slides. 
# DEFAULT is 10. If the number is large the report may be slow.
max_data_report = 10
```

Then, we cleaned the data to create dummy variables for the Departments.
```{r}
ProjectData <- read.csv(datafile_name)
ProjectData <- data.frame(ProjectData) 
## plot of the data by departments
ggplot(data=ProjectData, aes(x=satisfaction_level))+geom_bar(aes(fill=factor(left)))+ggtitle("Bar Chart by departments")+labs(x="Satisfaction scores", y="Count of records") +facet_wrap(~sales,  scales = "free")  

## only employees who left
ProjectData<-ProjectData[ProjectData$left==1,]
```

```{r}

##Replace all the salary from low medium high to 1 2 3
ProjectData$salary.f <- as.numeric(factor(ProjectData$salary,levels=c("low" ,"medium", "high")))

##Replace all the departments with dummy categorical variables
for(level in unique(ProjectData$sales)){
  ProjectData[paste("dummy", level, sep = "_")] <- ifelse(ProjectData$sales == level, 1, 0)
}
## remove the sales and salary columns
ProjectData$sales<-NULL
ProjectData$salary<-NULL

## write a CSV file of the new data
write.csv(ProjectData,"data/HR_clean.csv")

ProjectData_INITIAL <- ProjectData 
## Scale the data
ProjectData_scaled <- apply(ProjectData, 2, function(r) if (sd(r)!=0) (r-mean(r))/sd(r) else 0*r)

```
The light blue parts of the graph indicate people who stayed and the dark blue parts of the graph indicate people who left. We can note that in all departments, people who were extremely unhappy and had low satisfaction scores chose to leave the company. The number of people leaving declines with increasing satisfaction as can be expected. The department with the least turnover, is the management team, matching intuition that people who are typically recognized for their efforts through promotions are more satisfied. The accounting department closely follows this. The departments with the highest turnovers are arguably HR and support. Sales department has the highest percentage of retention of employees. Regardless of some patterns about employees leaving related to the departments they are in, we can note that satisfaction scores of employees vary in all departments leading to the turnover at certain critical points. Hence, there are other factors such as departments that can characterize employee turnover behavior and the rest of the analysis will help to identify them.

<hr>\clearpage

# Part 1: Key Customer Characteristics

From the data, we determine the key characteristics that identify the employees who left the firm. We are using the eigenvalue method to come up with the criterion to segment employees in multiple buckets.


```{r setupfactor, echo=FALSE, tidy=TRUE}
# Please ENTER then original raw attributes to use. 
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used = c(1:6,8,11:20)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "eigenvalue"

# Please ENTER the desired minumum variance explained 
# (Only used in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (Only used in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 5

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Default is "varimax"
rotation_used = "varimax"

```

```{r}
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData_scaled[,factor_attributes_used]
ProjectDataFactor <- ProjectData <- data.matrix(ProjectDataFactor)
```

## Steps 1-2: Check the Data 

We check the data that it is all metric data:

```{r}
rownames(ProjectDataFactor) <- paste0("Obs.", sprintf("%02i", 1:nrow(ProjectDataFactor)))
#iprint.df(t(head(round(ProjectDataFactor, 2), max_data_report)))
datatable(t(head(round(ProjectDataFactor, 2), max_data_report)), class = 'cell-border stripe')
```

We identified 16 parameters that define an employee in the data. These are satisfaction level, last evaluation completed, number of project, average monthly hours worked, time spend in the firm, work related accidents, number of promotions in last 5 years, and employee’s department (Accounting, HR, Technical, Support, Management, IT, Product Management, Marketing, and R&D).

Next, we depict the descriptive statistics of the 16 parameters with 0 being the mean.  


```{r}
#datatable(round(my_summary(ProjectDataFactor), 2))
iprint.df(round(my_summary(ProjectDataFactor), 2))
```

## Step 3: Check Correlations

We check the correlation between the 16 characteristics (parameters) identified in Step 1 & 2. From the below figure, it is evident that few characteristics are very much correlated to one another. For example, characteristics time spend in the firm, last evaluation completed, number of project and average monthly hours are strongly correlated to one another. This is quite evident as these 4 characteristics when bunched together signify “involvement” of an employee in the firm which could be inferred as time spent in firm or working. Second example could be correlation between satisfaction level and time spent in the firm. A positive correlation implies that a person who had spent enough time in the firm is more satisfied than the employee who is new in the firm. One of the hidden meaning from this correlation is that an employee who has spent less time in the firm has more chances of leaving the firm than an employee who has spent more time than the first employee.

The correlation would be useful in performing the factor analysis on the data to better define an ex-employee. 

```{r fig.align='center', fig=TRUE, bunch_o_figs_pointsize, fig.height=15, fig.width=15, dev.args=list(pointsize=18)}

##p-test function for the correlation matrix
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
p.mat <- cor.mtest(ProjectDataFactor)
## set up the colour template for the corrleation table
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(round(cor(ProjectDataFactor),2)
         , method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```

## Step 4: Choose number of factors

To further make the process easy to use and understandable, we would reduce the dimensions of the data i.e. reduce the variables (16 characteristics) identified in the previous step by performing the factor analysis. Factor analysis is a method to derive new and fewer components to better segment the data. 

The new factors will be the combination of 16 characteristics of the employees and will be 16 factors in total. First factor will explain most of the variance, second next and so on. Each factor will be associated with an eigenvalue. Eigenvalue corresponds to the amount of variance explained by each factor with standardized characteristics and each characteristic having a variance of 1. We will capture as much total variance as possible in as minimum factors as possible. 


```{r}
# Here is how the `principal` function is used 
UnRotated_Results<-principal(ProjectDataFactor, nfactors=ncol(ProjectDataFactor), rotate="none",score=TRUE)
UnRotated_Factors<-round(UnRotated_Results$loadings,2)
UnRotated_Factors<-as.data.frame(unclass(UnRotated_Factors))
colnames(UnRotated_Factors)<-paste("Comp",1:ncol(UnRotated_Factors),sep="")
```

```{r}
# Here is how we use the `PCA` function 
Variance_Explained_Table_results<-PCA(ProjectDataFactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table

rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table), sep=" ")
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")
```


```{r}
#datatable(round(Variance_Explained_Table, 2))
iprint.df(round(Variance_Explained_Table, 2))
```


The blue plot shows the decreasing eigenvalues with maximum 10 factors corresponding to the maximum eigenvalue of 1. This means that 10 factors signify the maximum characteristics of the employees. 
One of the ways to determine the number of factors to move forward is the “elbow” like shape in the plot. This is where the eigenvalue is one.

```{r}
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
iplot.df(melt(df, id="components"))
```


## Step 5: Interpret the factors


```{r}
if (factor_selectionciterion == "eigenvalue")
  factors_selected = sum(Variance_Explained_Table_copy[,1] >= 1)
if (factor_selectionciterion == "variance")
  factors_selected = 1:head(which(Variance_Explained_Table_copy[,"cumulative percentage of variance"]>= minimum_variance_explained),1)
if (factor_selectionciterion == "manual")
  factors_selected = manual_numb_factors_used
```

In this case we selected the `r rotation_used` rotation. For our data, the `r factors_selected` selected factors look as follows after this rotation: 

```{r}
Rotated_Results<-principal(ProjectDataFactor, nfactors=max(factors_selected), rotate=rotation_used,score=TRUE)
Rotated_Factors<-round(Rotated_Results$loadings,2)
Rotated_Factors<-as.data.frame(unclass(Rotated_Factors))
colnames(Rotated_Factors)<-paste("Comp.",1:ncol(Rotated_Factors),sep="")

sorted_rows <- sort(Rotated_Factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_Factors <- Rotated_Factors[sorted_rows,]

iprint.df(Rotated_Factors, scale=TRUE)
#datatable(Rotated_Factors)
```

To better visualize and interpret the factors we suppress loadings with small values, e.g. with absolute values smaller than 0.5. In this case our factors look as follows after suppressing the small numbers:

```{r}
Rotated_Factors_thres <- Rotated_Factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_Factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_Factors)

iprint.df(Rotated_Factors_thres, scale=TRUE)
```

After shortlisting the 10 factors based on the principal component analysis, we will try to define the factors based on the characteristics that are there in each factor. 

Component 1 (factor 1) consists of average monthly hours, last evaluation completed, number of projects completed and time spent in the firm. We will call this factor “Involvement” of an employee in the firm.

Component 2 (factor 2) consists of time spent in the firm and satisfaction level. We will call this factor “firm’s contribution to employee satisfaction”.

Component 3 (factor 3) consists of technical department and a support department (with negative weightage). We will call this factor “Technical department sans support function”.

Component 4 (factor 4) defines employee belonging to “IT department”.

Component 5 (factor 5) consists of Management department in the firm and number of promotions. This will signify “Promotions in Management”.

Component 6 (factor 6) defines employee not belonging to Human Resources and will call “Non-HR”.

Component 7 (factor 7) defines employee belonging to “Accounting”.

Component 8 (factor 8) defines employee belonging to “Marketing”.

Component 9 (factor 9) defines employee belonging to “Product Management”.

Component 10 (factor 10) defines employee belonging to “R&D”.


<hr>\clearpage

# Part 2: Customer Segmentation 

## Step 1. Select Segmentation Variables

The segementation variables chosen are based on the highest weights observed in the 10 components from the factor analysis
1) last evaluation
2) average monthly hours
3) departments of accounting, hr, support, management, IT, product management, marketing and R&D

```{r setupcluster, echo=FALSE, tidy=TRUE}
# Please ENTER then original raw attributes to use for the segmentation (the "segmentation attributes")
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
segmentation_attributes_used = c(2,4,11,12,14,15,16,17,18,19)  

# Please ENTER then original raw attributes to use for the profiling of the segments (the "profiling attributes")
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
profile_attributes_used = c(2:19)

# Please ENTER the number of clusters to eventually use for this report
numb_clusters_used = 3 # for boats possibly use 5, for Mall_Visits use 3

# Please enter the method to use for the segmentation:
profile_with = "hclust" #  "hclust" or "kmeans"

# Please ENTER the distance metric eventually used for the clustering in case of hierarchical clustering 
# (e.g. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" - see help(dist)). 
# DEFAULT is "euclidean"
distance_used = "euclidean"

# Please ENTER the hierarchical clustering method to use (options are:
# "ward", "single", "complete", "average", "mcquitty", "median" or "centroid").
# DEFAULT is "ward"
hclust_method = "ward.D"

# Please ENTER the kmeans clustering method to use (options are:
# "Hartigan-Wong", "Lloyd", "Forgy", "MacQueen").
# DEFAULT is "Lloyd"
kmeans_method = "Lloyd"

```

```{r}
# Same as the initial data
ProjectData <- ProjectData_INITIAL

segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectData))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectData))

ProjectData_segment <- ProjectData[,segmentation_attributes_used]
ProjectData_profile <- ProjectData[,profile_attributes_used]

ProjectData_scaled <- apply(ProjectData, 2, function(r) if (sd(r)!=0) (r-mean(r))/sd(r) else 0*r)
```


## Step 2: Define similarity measure

Using the euclidean distance metric as a measure, we used that ot define the differences in the observations of the profile of the employees.

```{r}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, max_data_report), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
rownames(euclidean_pairwise) <- colnames(euclidean_pairwise) <- sprintf("Obs.%02d", 1:max_data_report)

iprint.df(round(euclidean_pairwise))
```

## Step 3: Visualize Pair-wise Distances


```{r}
variables_to_plot = 1:2
do.call(iplot.grid, lapply(variables_to_plot, function(n){
  iplot.hist(ProjectData_segment[, n], breaks=5, xlab = paste("Variable", n))
}))
```

Below shows histogram of all pairwise distances for the `r distance_used` distance:

```{r}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
iplot.hist(Pairwise_Distances, breaks=10)
```

## Step 4: Method and Number of Segments


Below we have the dendogram of our Hierachical Clustering of our data.

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
iplot.dendrogram(Hierarchical_Cluster)
# TODO: Draw dendogram with red borders around the 3 clusters
# rect.hclust(Hierarchical_Cluster, k=numb_clusters_used, border="red") 
```

Displayed also is a plot of the distances of travelled for the points. 
```{r}
num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

We chose 3 segements to segregate our dataset.

Here is the segment membership of the first `r max_data_report` respondents if we use hierarchical clustering:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, k=numb_clusters_used)) # cut tree into 3 clusters
cluster_ids_hclust=unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

iprint.df(round(head(ProjectData_with_hclust_membership, max_data_report), 2))
```

while this is the segment membership if we use k-means:

```{r}
kmeans_clusters <- kmeans(ProjectData_segment,centers= numb_clusters_used, iter.max=2000, algorithm=kmeans_method)

ProjectData_with_kmeans_membership <- cbind(1:length(kmeans_clusters$cluster),kmeans_clusters$cluster)
colnames(ProjectData_with_kmeans_membership)<-c("Observation Number","Cluster_Membership")

iprint.df(round(head(ProjectData_with_kmeans_membership, max_data_report), 2))
```

## Step 5: Profile and interpret the segments 

The table below uses the 10 variables to segment employees by showing the average of all the input variables in each segment of employee compared to the ratio of the average of all the employees using the ratio of the two.

```{r}
cluster_memberships_kmeans <- kmeans_clusters$cluster 
cluster_ids_kmeans <- unique(cluster_memberships_kmeans)

if (profile_with == "hclust"){
  cluster_memberships <- cluster_memberships_hclust
  cluster_ids <-  cluster_ids_hclust  
}
if (profile_with == "kmeans"){
  cluster_memberships <- cluster_memberships_kmeans
  cluster_ids <-  cluster_ids_kmeans
}

# WE WILL USE THESE IN THE CLASSIFICATION PART LATER
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Seg.", 1:length(cluster_ids), sep="")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

iprint.df(round(cluster.profile, 2))
```

As can be seen from the segmentation, broadly the defining variable for each of the segment is the number of hours spent in the company and the number of projects undertaken, this has led to the formation of 3 distinct segments with distinct identities.

The snake plot for the 3 segments across the 19 variables on the basis of the means of each variable across each independent segment highlights the critical variables where the 3 segments of employees differ in their behavior, Time Spent in Company, Number of Projects, Average Monthly hours and Last evaluation


```{r}
ProjectData_scaled_profile = ProjectData_scaled[, profile_attributes_used,drop=F]

Cluster_Profile_standar_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_scaled_profile[(cluster_memberships==i), ,drop = F], 2, mean))
if (ncol(ProjectData_scaled_profile) < 2)
  Cluster_Profile_standar_mean = t(Cluster_Profile_standar_mean)
colnames(Cluster_Profile_standar_mean) <- paste("Seg ", 1:length(cluster_ids), sep="")

iplot.df(melt(cbind.data.frame(idx=as.numeric(1:nrow(Cluster_Profile_standar_mean)), Cluster_Profile_standar_mean), id="idx"), xlab="Profiling variables (standardized)",  ylab="Mean of cluster")
```

On comparing the average of each of the profiling variable of each of the segment against the mean of the population for each variable, we get that all 3 segments are inherently distinct, with segment 1 standing for employees who spent the least time in the company, segment 2 stands for employees who undertook the maximum projects as compared to the population mean and segment 3 stands for employees with the least number of promotions as compared to the population


```{r}
population_average_matrix <- population_average[,"Population",drop=F] %*% matrix(rep(1,ncol(Cluster_Profile_mean)),nrow=1)
cluster_profile_ratios <- (ifelse(population_average_matrix==0, 0,Cluster_Profile_mean/population_average_matrix))
colnames(cluster_profile_ratios) <- paste("Seg.", 1:ncol(cluster_profile_ratios), sep="")
rownames(cluster_profile_ratios) <- colnames(ProjectData)[profile_attributes_used]
## printing the result in a clean-slate table
iprint.df(round(cluster_profile_ratios-1, 2))
```

Hence, through this robust segmentation exercise, the employees who left the organization can broadly be divided into 3 segments, Segment 1 – Employees who spent the least time in the organization, Segment 2 – Employees who undertook the maximum projects in the year and spent the highest average monthly hours in the company, and Segment 3 – Employees who had the least number of promotions in the group in the last 5 years	 

# Conclusion

Our analysis revealed three key segments in the company. These include Segment 1 – Employees who spent the least time in the organization, Segment 2 – Employees who undertook the maximum projects in the year and spent the highest average monthly hours in the company, and Segment 3 – Employees who had the least number of promotions in the group in the last 5 years. These segments represent the three profiles that describe employees who have left the firm.
 
Based on our analysis, we cannot conclude with absolute certainty that employees who match one or more of these three profiles will leave the firm. Instead, we would recommend the following actions to gain further insight into attrition:
 
1. HR could consider monitoring current employees who match one or more of these segment profiles. This would help managers look for and recognize early signs related to an employee’s desire to leave the firm. Being prepared to persuade the employee to stay could empower managers to retain their staff. Therefore, the results of this study may allow HR to play a more proactive supporting role in retaining employees.

2. The firm may also want to examine the turnover patterns across departments. The current data analysis does not provide detail needed below the departments, but it is interesting that HR and sales appear to have higher attrition as compared to other departments at the firm. Perhaps staff interviews or gathering more department specific data to analyze would be helpful.

3. Further study is needed to complement the findings in this study and understand the drivers behind why employees choose to leave the firm. For example, perhaps further analysis complemented with interviews will reveal that instituting new policies around work/life balance would reduce turnover, if burnout is a common cause for an employee to leave the firm.
 
Should the company want to iterate upon this initial study, increasing the number of factors in subsequent analysis would improve accuracy and the level of detail available to describe patterns related to the firm’s attrition.
 
 
<hr>\clearpage